from google.colab import drive
drive.mount('/content/drive')
import tensorflow as tf
from tensorflow.keras import models, layers
import matplotlib.pyplot as plt
import numpy as np
import os
IMAGE_SIZE = 224
BATCH_SIZE = 32
EPOCHS = 15
CHANNELS=3
#Update the path to your dataset directory.
# If it's in your Google Drive, it might look like '/content/drive/MyDrive/dataset'
dataset_path = "/content/drive/MyDrive/Knee training,validation and testing"

# Manually count the images in each class directory
class_image_counts = {class_name: len(os.listdir(os.path.join(dataset_path, class_name)))
                      for class_name in os.listdir(dataset_path)
                      if os.path.isdir(os.path.join(dataset_path, class_name))}

print("Image counts per class:", class_image_counts)

# Assign the result of the function call to the variable 'dataset'
dataset = tf.keras.preprocessing.image_dataset_from_directory(
    dataset_path,  # Use the correct path to your dataset
    shuffle=True,
    image_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE
)
num_batches = len(dataset)
print(f"Number of batches: {num_batches}")
total_images = 0
for batch in dataset:
    total_images += batch[0].shape[0]  # batch[0] are the images

print(f"Total number of images: {total_images}")

class_name = dataset.class_names # Now you can use 'dataset'
print(class_name)

# Number of images (approximation)
num_images = total_images
print(f"Approximate total number of images: {num_images}")
Output:
Image counts per class: {'training': 2, 'testing': 2, 'validation': 2}
Found 372 files belonging to 3 classes.
Number of batches: 12
Total number of images: 372
['testing', 'training', 'validation']
Approximate total number of images: 372

for image_batch, labels_batch in dataset.take(1):
    pass
for image_batch, labels_batch in dataset.take(1):
  for i in range(min(image_batch.shape[0], 12)):  # Limit to 12 or fewer images
    ax = plt.subplot(3, 4, i + 1)
    plt.imshow(image_batch[i].numpy().astype("uint8"))
    plt.title(class_name[labels_batch[i]])
    plt.axis("off")
plt.show()
img_height, img_width = (224, 224)
batch_size = 32

train_data_dir = r"/content/drive/MyDrive/Knee training,validation and testing/training"
valid_data_dir = r"/content/drive/MyDrive/Knee training,validation and testing/validation"
test_data_dir = r"/content/drive/MyDrive/Knee training,validation and testing/testing"
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf  # If tf is not already imported
# If you plan to use the 'preprocess_input' function
# Ensure it's imported from the correct module
from tensorflow.keras.applications.vgg16 import preprocess_input

# ... your existing code ...

train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,
                                   shear_range=0.2,
                                   zoom_range=0.2,
                                   horizontal_flip=True,
                                   validation_split=0.4)

# Define train_generator
train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical',
    subset='training'  # Use 'training' for the training set
)

valid_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation'
)

test_generator = train_datagen.flow_from_directory(
    test_data_dir,
    target_size=(img_height, img_width),
    batch_size=1,
    class_mode='categorical'
    # Remove subset='validation' as it's not needed for test data
)
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential


resize_and_rescale = tf.keras.Sequential([
     layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),
     layers.Rescaling(1.0/255)
])
data_augmentation = tf.keras.Sequential([
     layers.RandomFlip("horizontal_and_vertical"),
     layers.RandomRotation(0.2),
])

x, y = next(test_generator) # Use next(test_generator) to get the next batch
x.shape
import tensorflow as tf
from tensorflow import keras
from keras.applications import ResNet50
from keras.layers import Dense, Dropout, GlobalAveragePooling2D
from keras.models import Model
base_model =  ResNet50(weights = 'imagenet', include_top = False, input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3))
#freezing the layers
for layer in ResNet.layers:
  layer.trainable = False
input_shape = (None,IMAGE_SIZE, IMAGE_SIZE,3)
model = keras.Sequential([
    keras.layers.GlobalAveragePooling2D(),
    keras.layers.Dense(1024, activation = 'relu'),
    keras.layers.Dense(512, activation = 'relu'),
    keras.layers.Dense(train_generator.num_classes, activation = 'softmax')
])
model.build(input_shape = input_shape)

model.summary()

model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

from tensorflow.keras.callbacks import EarlyStopping
callback = EarlyStopping(
    monitor = 'val_loss',
    min_delta = 0.00001,
    patience = 20,
    baseline = None,
    verbose = 1,
    mode = "auto",
    restore_best_weights = False
)
import tensorflow as tf

# Assuming 'train_data_dir' is a path to your image data directory
train_dataset = tf.keras.utils.image_dataset_from_directory(
    train_data_dir,
    labels='inferred', # Infer labels from subdirectory names
    label_mode='categorical', # Use categorical labels for multi-class classification
    image_size=(224, 224), # Resize images to match ResNet50 input shape
    batch_size=32, # Match your desired batch size
    shuffle=True # Shuffle the data
)

# ... (rest of your code)

history = model.fit(
    train_dataset, # Pass the TensorFlow Dataset object
    validation_data=valid_generator,
    verbose=1,
    epochs=15,
    callbacks=[callback],
)










